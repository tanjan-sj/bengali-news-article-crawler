{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crawler_test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2tEzB11I9sP",
        "colab_type": "text"
      },
      "source": [
        "*python 3.0* news article crawler using **requests** and **beautifulsoup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqYz7nd-JJen",
        "colab_type": "code",
        "outputId": "7b944ce1-e6f6-46b0-e0cb-7200ac5c995b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import requests\n",
        "import json\n",
        "import hashlib\n",
        "import os\n",
        "from google.colab import files\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def compute_MD5_hash(string):\n",
        "\n",
        "  m = hashlib.md5()\n",
        "  m.update(string.encode('utf-8'))\n",
        "  return m.hexdigest()\n",
        "\n",
        "\n",
        "def get_soup(request_url):\n",
        "\n",
        "  page = requests.get(request_url)\n",
        "  soup = BeautifulSoup(page.text, 'html.parser')\n",
        "  return soup\n",
        "\n",
        "\n",
        "def get_all_news_url(soup):\n",
        "\n",
        "  news_list = soup.find_all(\"div\", class_=\"article news-bn first default \")\n",
        "  article_url_list = []\n",
        "    \n",
        "  for i in range(len(news_list)):\n",
        "    for link in news_list[i].find_all('a'):\n",
        "      article_url_list.append(link.get('href'))\n",
        "\n",
        "  return article_url_list\n",
        "\n",
        "\n",
        "def get_all_titles(articles):\n",
        "  \n",
        "  titles = []\n",
        "\n",
        "  for i in range(len(articles)):\n",
        "    page = requests.get(articles[i])\n",
        "    soup = BeautifulSoup(page.text, 'html.parser')\n",
        "    title_list = soup.find('h1', class_ =' print-only')\n",
        "    titles.append(title_list.string)\n",
        "\n",
        "  return titles\n",
        "\n",
        "\n",
        "def get_all_datetime(articles):\n",
        "\n",
        "  datetime_list = []\n",
        "\n",
        "  for i in range(len(articles)):\n",
        "    page = requests.get(articles[i])\n",
        "    soup = BeautifulSoup(page.text, 'html.parser')\n",
        "    date_temp = soup.find('p', class_ = 'dateline print-only')\n",
        "    date_temp_text = date_temp.get_text()\n",
        "    date_temp_text_final = date_temp_text[11:33]\n",
        "    date_temp_text_final.strip()\n",
        "    datetime_list.append(date_temp_text_final)\n",
        "  \n",
        "  return datetime_list\n",
        "\n",
        "\n",
        "def get_primary_contents(articles):\n",
        "  \n",
        "  primary_contents = []\n",
        "\n",
        "  for i in range(len(articles)):\n",
        "    page = requests.get(articles[i])\n",
        "    soup = BeautifulSoup(page.text, 'html.parser')\n",
        "    bold_content = soup.find(\"meta\",  property=\"og:description\")\n",
        "    primary_contents.append(bold_content.get(\"content\"))\n",
        "\n",
        "  return primary_contents\n",
        "\n",
        "\n",
        "def get_full_article_data(articles, primary_contents):\n",
        "\n",
        "  list_article = []\n",
        "\n",
        "  for i in range(len(articles)):\n",
        "    page = requests.get(articles[i])\n",
        "    soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "    article_body = soup.find('div', class_ = 'custombody print-only')\n",
        "    x = article_body.find_all('p')\n",
        "    article = ''\n",
        "    final_article = ''\n",
        "    list_paragraphs = []\n",
        "\n",
        "    for j in range(len(x)):\n",
        "      temp = x[j].text.strip()\n",
        "      temp = temp.replace('\\n', ' ')\n",
        "      list_paragraphs.append(temp)\n",
        "    article = \"\".join(list_paragraphs)\n",
        "\n",
        "    final_article = primary_contents[i] + \"\\n\" + article\n",
        "    list_article.append(final_article)\n",
        "\n",
        "  return list_article\n",
        "\n",
        "\n",
        "def get_json_file(articles, titles, datetime_list, list_article):\n",
        "\n",
        "  md5hash_list = []\n",
        "\n",
        "  for i in range(len(articles)):\n",
        "    #print(titles[i], ':', datetime_list[i], '\\n', list_article[i])\n",
        "    json_file = {'title': titles[i], 'date': datetime_list[i], 'content': list_article[i]}\n",
        "    date_local_directory = datetime_list[i].strip()\n",
        "    hashed_url = compute_MD5_hash(articles[i]).upper()\n",
        "\n",
        "    local_directory_format =  hashed_url + '.json'\n",
        "\n",
        "    with open(local_directory_format, 'w', encoding='utf-8') as f:\n",
        "      json.dump(json_file, f, ensure_ascii=False)\n",
        "    \n",
        "    if hashed_url in md5hash_list:\n",
        "      break\n",
        "    else:\n",
        "      md5hash_list.append(hashed_url)\n",
        "    \n",
        "    files.download(local_directory_format)\n",
        "\n",
        "\n",
        "request_url = 'https://bangla.bdnews24.com/'\n",
        "soup = get_soup(request_url)\n",
        "article_url_list = get_all_news_url(soup)\n",
        "list_article = get_all_article_data(article_url_list)\n",
        "titles = get_all_titles(article_url_list)\n",
        "datetime_list = get_all_datetime(article_url_list)\n",
        "primary_contents = get_primary_contents(article_url_list)\n",
        "list_article = get_full_article_data(article_url_list, primary_contents)\n",
        "\n",
        "get_json_file(article_url_list, titles, datetime_list, list_article)\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPcZzCmvsf7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdiE380H-McH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}